\documentclass[a4paper, 12pt]{article}

%%%%%%%%%%%%
% Packages %
%%%%%%%%%%%%

\usepackage[english]{babel}
\usepackage[noheader]{packages/sleek}
\usepackage{packages/sleek-title}
\usepackage{packages/sleek-theorems}
\usepackage{packages/sleek-listings}
\usepackage{hyperref}
\usepackage{tcolorbox}
\usepackage[sorting=none]{biblatex}
\usepackage{epstopdf}

\usepackage{enumitem}

\usepackage{amssymb}
\usepackage{pifont}
\newcommand{\xmark}{\ding{55}} % cross mark


\usepackage{graphicx}


\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{4}
%%%%%%%%%%%%%%
% Title-page %
%%%%%%%%%%%%%%

\logo{figures/multitel_logo.eps}
\institute{AI Department}
% \faculty{Faculty of Whatever Sciences}
%\department{Department of Anything but Psychology}
\title{Latent Spaces, Embeddings and Representations}
% \subtitle{With a sleeker title-page}
\author{\textit{Author}\\Horacio \textsc{Tellez}}
%\supervisor{Linus \textsc{Torvalds}}
%\context{Well, I was bored...}
\date{\today}

%%%%%%%%%%%%%%%%
% Bibliography %
%%%%%%%%%%%%%%%%

\addbibresource{bibliography.bib}

%%%%%%%%%%
% Others %
%%%%%%%%%%


\newtheorem{remark}{Remark}


% Define the custom environment
\newenvironment{hangingdescription}{%
  % Code to execute when the environment starts
  \begin{description}[
    align=left,
    leftmargin=\parindent, % Adjust this as needed for desired indentation
    labelsep=0.5em,       % Adds a colon after the label
    font=\normalfont\bfseries % Makes the label bold
  ]%
}{%
  % Code to execute when the environment ends
  \end{description}%
}


\lstdefinestyle{latex}  {
    language=Tex,
    style=default,
    %%%%%
    commentstyle=\ForestGreen,
    keywordstyle=\TrueBlue,
    stringstyle=\VeronicaPurple,
    emphstyle=\TrueBlue,
    %%%%%
    emph={LaTex, usepackage, textit, textbf, textsc}
}

\FrameTBStyle{latex}

\def\tbs{\textbackslash}

%%%%%%%%%%%%
% Document %
%%%%%%%%%%%%






% Custom Math Operators (example)
\DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\spec}{spec}

% Custom macros (example)
\newcommand{\R}{\mathbb{R}} % Real numbers
\newcommand{\C}{\mathbb{C}} % Complex numbers
\newcommand{\N}{\mathbb{N}} % Natural numbers
\newcommand{\Z}{\mathbb{Z}} % Integers
\newcommand{\Q}{\mathbb{Q}} % Rational numbers
\newcommand{\A}{\mathcal{A}} % Calligraphic A
\newcommand{\F}{\mathcal{F}} % Calligraphic F









\date{\today} % Or \date{} for no date (arXiv adds its own date)

% --- Main Document ---
\begin{document}

\maketitle % Generates the title, author, and date

\begin{abstract}
    This document will compile the ideas and current state of the art about latent spaces,
    embeddings and representations. We will focus in subjects that touch AI research somehow,
    even if this is purely a mathematical subject.
\end{abstract}

% Keywords (optional, but good for searchability)
\noindent\textbf{Keywords:} Latent space, embeddings, representations.

% MSC2020 Classification (optional, but highly recommended for math articles)
% You can find codes at: https://mathscinet.ams.org/mathscinet/msc/msc2020.html
% add or remove this
% \noindent\textbf{MSC2020:} 12X34, 56Y78, 90Z01. % Example codes, replace with your own

% \tableofcontents % Generates a table of contents. Optional, especially for shorter papers.
% \newpage % Starts the content on a new page after the TOC.

% --- Main Content ---
% --------------------------------------------------------------------






\section{Information Quantification}

An important concept, even if our knowledge of latent space and embeddings is meager, is how well our transformation maps the data from one space to another. 
The question itself can be approached from several directions and each of these directions have different answers.

One idea is about quantity of information in the data. One good transformation should preserve as much information as possible. One way of measuring 
such is Shannon Entropy \cite{shannon1948mathematical}.
Give a random variable $X$ with probability distribution $p : \mathcal{X} \rightarrow [0, 1]$ , then the Shannon entropy $H(X)$ is defined by:
\begin{equation}\label{eq:shannon_entropy_integral}
 H(X) = - \int_{\mathcal{X}} p(x) \log p(x).
\end{equation}
Of course, in case $\mathcal{X}$ is a discrete space, the entropy is reduced to
\begin{equation}\label{eq:shannon_entropy_sum}
 H(X) = -  \sum_{x \in \mathcal{X}} p(x) \log p(x).
\end{equation}
This is for cases where we have access to the probability distribution $p$. When this is not the case, and we only have a sampled population, we need to estimate $p$ somehow.
One immediate issue on estimating entropy on discrete and sampled data is how to choose hyperparameters 
as bins, one work on this is \cite{beirlant1997nonparametric}. Below we give a shallow study of how to gain $\hat{p}$, estimates of $p$, which can then be used to find $\hat{H}$, estimates of $H$. We begin by methods of estimating $p$, and then we delve into how fine-tune those methods for better estimates.

\subsection{Estimators}
\subsubsection{Histogram}
We can, from an observation $(x_1, \dots, x_n)$ from $p$ build and histogram-based estimator for $p$. 
Let $A_1, \dots, A_l$ be a cover family of bins, the estimator is defined by
\begin{equation}\label{eq:histogram_estimator}
   \hat{p}(x) = \sum_{1}^l \frac{ \# A_j  }{n * |A_j|} \mathcal{1}_{A_j}(x)
\end{equation}
where $\#A_j$ is the number of elements in bin $A_j$ and $|A_j|$ is the size of the bin. One of the common choises is to have equal sized bins, for example $|A_j| = M$ for some $M$ for all $j$.
Under the hypothesis that $p'$ exists and $\sup |p'(x)| < L$ for some $L$, we have that there exists an \textit{optimal} choice for the size of bins:
\begin{equation}
    M_{\text{opt}} = \left( \frac{n * L^2}{p(x^*)} \right)^{\frac{1}{3}}
\end{equation}
where $x^*$ is given by the mean theorem for each bin. $M_{\text{opt}}$ is optimal in the sense that it minimses the bias and variance at the same time. More specifically, it is the answer of the minimization problem:
\begin{equation}
    MSE(\hat{p}_M(x)) = bias²(\hat{p}_M(x)) + Var(\hat{p}_M(x)).
\end{equation}


\subsubsection{KDE}

\paragraph{Kernel Function}
The kernel functions that are commonly used on statistics and deep learning all come from a group of properties defined on a higher dimensional Hilbert Space. More specifically, a kernel function is $k: \mathbb{H} \times \mathbb{H} \rightarrow \mathbb{R}$ such that 
$k(x, x') = k(x', x)$ and the \emph{Gram} matrix is Positive Semi-Definite. What is called a kernel function is then $K(x) := \sqrt{k(x,x)}$. This definition for $K$ implies that it is symmetric and positive, usually we normalize $K$ such that $\int K(x)dx = 1$  and can be treated as a distribution. 
This is the case for example for the gaussian kernel which is $K(x) = e^{-\frac{1}{2}\left( \frac{x - \mu}{\sigma} \right)^2}$ but is usually presented in its normalized version $K(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2}\left( \frac{x - \mu}{\sigma} \right)^2}$.
It is also the case that it is sometimes asked for the kernel to drop at infinity, that is:
$\lim_{x \rightarrow \infty} K(x) = \lim_{x \rightarrow - \infty} K(x) = 0$.

There are nevertheless situations where is interesting to relax the conditions on the kernel and either drop or modify some of them.
The dynamic warping time (DWT \cite{sakoe1978dynamic}) approach is a form of kernel where \emph{semi-definiteness} is dropped, also similarity scores do not possess this property.
Transition probabilities in Markov chains are also a type of kernel, but symmetry is dropped. And negative kernels are found in conflict-based measures.

\paragraph{Most Used Kernels}
There is a whole family of kernels, maybe the most known is the gaussian. The ones that are more commonly used are the following:
\begin{itemize}
    \item Gaussian: 
        \begin{equation}
            K(x) = \frac{1}{\sqrt{2\pi}\sigma} e^{- \frac{1}{2} \left( \frac{x - \mu}{\sigma}\right)^2 }
        \end{equation}
    \item Uniform:
        \begin{equation}
           K(x) = \frac{1}{2} \mathbb{1}_{[-1, 1]}(x) 
        \end{equation}
    \item Epanechnikov:
        \begin{equation}
            K(x) = \frac{3}{4} \max \{ 1 - x^2, 0 \}
        \end{equation}
\end{itemize} 
\remark{} The Epanechnikov kernel, additionally to begin fairly easy to compute, is the one with the lowest asymptotic MSE.

\paragraph{Estimator}
The kernel density estimator (KDE) is used to approximate or estimate and unkown ditribution $p$ from $n$ observations $(x_1, \dots, x_n)$. The KDE is defined by
\begin{equation}\label{eq:kde_def}
   \hat{p}_{K, h}(x) = \frac{1}{nh} \sum_{1}^n K \left( \frac{x - x_i}{h} \right).
\end{equation}
Where $K$ is a kernel function and $h > 0$ is the \textit{smoothing} factor.

The KDE is a good estimator, that is, is has strong consistency (see \cite{beirlant1997nonparametric}) when using it for 1D signals and the following expression:
\begin{equation}\label{eq:entropy_kde_estimator}
   H_n = - \int_{A_n} \hat{p}(x) \log \hat{p}(x)
\end{equation}
where $A_n = [-b_n, b_n]$ is a family of ever-increasing intervals. This is under the hypothesis that $p$ is tail decreasing.

\subsubsection{Optimal Bandwidth and Bandwidth Estimators}

In the case of the KDE estimator, the hyperparameter to tune or choose is the bandwidth $h$. In that case we can try to minimise the \textit{Integrated Square Error (ISE)}:
\begin{equation}\label{eq:ise}
   ISE\left[ \hat{p}_{K, h} \right]  = \int (\hat{p}_{K, h}(x) - p(x))^2 dx
\end{equation}
this ISE value is dependent on two hyperparameters: $n$ the sample size and the bandwidth $h$, to remove the sample size as a parameter we can
pass to the \textit{Mean ISE (MISE)}:
\begin{equation}\label{eq:mise}
\begin{split}
    MISE \left[ \hat{p}_{K, h} \right] & = \mathbb{E} \left[  ISE [ \hat{p}_{K, h} ] \right] \\
     & = \mathbb{E} \left[  \int (\hat{p}_{K, h}(x) - p(x))^2 dx \right] \\
     & = \int \mathbb{E} \left[ (\hat{p}_{K, h}(x) - p(x))^2 \right] dx \\
     & = \int MSE \left[  \hat{p}_{K, h} \right].
\end{split}
\end{equation}

In this case we can find a bandwidth $h$ that minimizes the MISE (after a lot of computations):
\begin{equation}\label{eq:mise_minimiser}
   h_{\text{opt}} = \left[ \frac{\int K(x)^2 dx }{ \left( \int x^2 K(x) dx \right) \left( \int (p''(x)) dx \right)} \right]^{\frac{1}{5}}
\end{equation}
Note that we cannot compute this optimal value, as we do not have access to $\int (p''(x)) dx$. We can use, however, several plug-in rules for 
approximately computing $h_{\text{opt}}$.

\paragraph{Normal Scale Bandwith Selector}

If we suppose that $p$ follows a normal law $\mathcal{N}(\mu, \sigma^2)$, then, through finding an estimator $\hat{\sigma}$ of the variance $\sigma$, we get
\begin{equation}\label{eq:mise_minimiser_ns_estimator}
   \hat{h}_{NS} = \left[ \frac{8 \pi^\frac{1}{2} \left(\int K(x)^2 dx\right) }{3n \int x^2K(x)dx } \right]^\frac{1}{5} \hat{\sigma}
\end{equation}

\paragraph{Normalized Normal Kernel}
If the kernel $K$ used as estimator in the KDE has the following properties:
\begin{itemize}
    \item $\int x^2 K(x) dx = 1$;
    \item $\int (K(x))^2 dx = \frac{1}{2 \sqrt{\pi}}$
\end{itemize}
Then the normal scale bandwidth $h_{NS}$ can be reduced to
\begin{equation}
   \hat{h}_{NNK} = \left( \frac{4}{3} \right)^{\frac{1}{5}} n^{- \frac{1}{5}} \hat{\sigma} \approx 1.06 n^{- \frac{1}{5}} \hat{\sigma}
\end{equation}

\paragraph{Direct Plug-In}
This estimator is harder to arrive to, but has (as we will see later) higher fidelity and convergence rate. The processus is the following
\begin{itemize}
    \item find an estimator $\hat{\sigma}$ of $\sigma$ if we suppose that $p \sim \mathcal{N}(\mu, \sigma)$;
    \item define $\hat{\psi_8} = \frac{105}{32 \sqrt{\pi \hat{\sigma}^9}}$; (this is in fact an estimate of $\psi_8$, an integrating polynomial for $\int p^{(2)}(x)dx$, details are in \cite{garcia-portugues2021kde}: 2.4 \textit{Bandwidth Selection})
    \item define $g_1 = \left[ - \frac{2K^{(6)}(0)}{n \hat{\psi_8} \int x^2 K(x) dx} \right]^{\frac{1}{9}}$;
    \item  define $g_2 = \left[ - \frac{2K^{(4)}(0)}{n \hat{\psi_6}(g_1) \int x^2 K(x) dx} \right]^{\frac{1}{7}} $;
\end{itemize}
The \textit{Direct Plug-In (DPI)} estimator is then
\begin{equation}\label{eq:h_dpi_def}
   \hat{h}_{DPI} = \left[ \frac{\int (K(x))^2 dx }{ n\hat{\psi_4}(g_2)n \int x^2K(x)dx} \right]^{\frac{1}{5}} 
\end{equation}


\paragraph{Cross-Validation}
We can estimate $h_{\text{opt}}$ by optimizing against part of the sampled population. That is, use part of the population to estimate the MISE and another part to validate the error. This can be done through the minimization of \textit{Least Squares Cross-Validation (LSCV)}:
\begin{equation}
    LSCV(h) := \int (\hat{p}_{K, h})^2 dx - 2n^{-1} \sum_{i=1}^n \left( \frac{1}{n-1} \sum_{j \neq i} \frac{1}{h}K\left( \frac{x - x_j}{h} \right) \right).
\end{equation}
Thus, we get another candidate for estimating $h_{\text{opt}}$:
\begin{equation}\label{eq:h_lscv_def}
   \hat{h}_{LSCV} = argmin_{h > 0} LSCV(h).
\end{equation}
Unfortunately, $\hat{h}_{LSCV}$ can only be estimated through numerical means, no closed formulae.


\subsubsection{Resources}
This \href{https://bookdown.org/egarpor/NP-UC3M}{site} has a good compilation of established results on statistics and estimators: \cite{garcia-portugues2021kde}, the reader can also go see \cite{chen2019density}


\section{Interesting Bits}

\subsection{Mean Squared Error (MSE)}

The MSE that is commonly used on deep learning and more generally in machine learning has a theoretical foundation as to why is a good loss
function.

Let \( \hat{\theta} \) be an estimator of a parameter \( \theta \). The \textbf{mean squared error (MSE)} of \( \hat{\theta} \) is defined as:

\[
\text{MSE}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \theta)^2\right]
\]

This measures the average squared difference between the estimator and the true value.

\subsubsection{Bias and Variance}

The bias of \( \hat{\theta} \) is:

\[
\text{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta
\]

The variance of \( \hat{\theta} \) is:

\[
\text{Var}(\hat{\theta}) = \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2\right]
\]

\subsubsection{Bias-Variance Decomposition}

We can decompose the MSE into the squared bias and the variance:

\[
\begin{aligned}
\text{MSE}(\hat{\theta}) 
&= \mathbb{E}\left[(\hat{\theta} - \theta)^2\right] \\
&= \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}] + \mathbb{E}[\hat{\theta}] - \theta)^2\right] \\
&= \mathbb{E}\left[(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2\right] + (\mathbb{E}[\hat{\theta}] - \theta)^2 \\
&= \text{Var}(\hat{\theta}) + \text{Bias}(\hat{\theta})^2
\end{aligned}
\]

This identity shows the trade-off between bias and variance. Lowering one may increase the other, and minimizing MSE often involves balancing both.

\subsubsection{Application: Histogram-Based Estimators}

Suppose we are estimating a functional of a density \( f(x) \), such as entropy \( H(f) \), using a histogram-based estimator \( \hat{f}_n \).

The estimator introduces two sources of error:

\begin{itemize}
    \item \textbf{Bias:} When bin width \( h_n \) is too large, the histogram smooths over important features of \( f(x) \), increasing bias.
    \item \textbf{Variance:} When \( h_n \) is too small, there are fewer samples per bin, making the estimate unstable and increasing variance.
\end{itemize}

The MSE of the estimator \( \hat{H}_n \) for entropy is then:

\[
\text{MSE}(\hat{H}_n) = \text{Bias}(\hat{H}_n)^2 + \text{Var}(\hat{H}_n)
\]

Choosing an optimal bin width \( h_n \) (or bandwidth in kernel estimators) minimizes this MSE.

\subsubsection{Conclusion}

The bias-variance decomposition provides a fundamental lens for analyzing estimator quality. It underpins the design of statistical procedures that must manage the trade-off between approximation accuracy (bias) and stability (variance).


\subsection{Statistical MSE vs Deep Learning MSE loss function}

In classical statistics, the \textbf{mean squared error (MSE)} of an estimator \( \hat{\theta} \) for a parameter \( \theta \) is defined as:

\[
\text{MSE}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \theta)^2] = \underbrace{(\mathbb{E}[\hat{\theta}] - \theta)^2}_{\text{Bias}^2} + \underbrace{\mathbb{V}[\hat{\theta}]}_{\text{Variance}}
\]

This decomposition expresses the MSE as the sum of the squared bias and the variance of the estimator.

\subsubsection{Supervised Learning Setting}

In supervised learning (e.g., regression), we are given data \( \{(x_i, y_i)\}_{i=1}^n \) drawn i.i.d. from an unknown distribution \( P_{X,Y} \). We want to learn a function \( f_\theta(x) \), parameterized by \( \theta \), that predicts \( y \) from \( x \).

The goal is to find \( f_\theta \) that minimizes the expected prediction error:

\[
\mathcal{R}(\theta) = \mathbb{E}_{(X,Y)} \left[ (f_\theta(X) - Y)^2 \right]
\]

This is the \textbf{expected risk} under the squared loss. It is the statistical MSE between the prediction \( f_\theta(X) \) and the true output \( Y \).

\subsubsection{Empirical Risk and the MSE Loss Function}

Since the distribution \( P_{X,Y} \) is unknown, we cannot compute the expectation directly. Instead, we approximate it using the \textbf{empirical risk} over the training data:

\[
\hat{\mathcal{R}}_n(\theta) = \frac{1}{n} \sum_{i=1}^n (f_\theta(x_i) - y_i)^2
\]

This empirical risk is the \textbf{mean squared error loss function} used in deep learning. Training a model involves minimizing this quantity:

\[
\hat{\theta} = \arg\min_{\theta} \hat{\mathcal{R}}_n(\theta)
\]

\subsubsection{From Statistical MSE to MSE Loss in Deep Learning}

Thus, the MSE loss function used in deep learning is an \textbf{empirical estimator} of the expected MSE from classical statistics:

\[
\text{Statistical MSE:} \quad \mathbb{E}[(f_\theta(X) - Y)^2] 
\quad \longrightarrow \quad
\text{Empirical MSE Loss:} \quad \frac{1}{n} \sum_{i=1}^n (f_\theta(x_i) - y_i)^2
\]

As \( n \to \infty \), and under regularity conditions, the empirical loss converges to the expected risk due to the law of large numbers. Therefore, minimizing the empirical MSE approximates minimizing the true MSE, which justifies using it as a loss function.

\subsubsection{Summary}

The deep learning MSE loss function is a practical, data-driven version of the theoretical MSE. It bridges statistical estimation and machine learning by turning expectations into sample averages, making them computationally feasible to optimize with gradient-based methods.


\subsection{The Stein Paradox}

The \textbf{Stein Paradox} is a remarkable result in statistical estimation that shows, under certain conditions, that the most intuitive estimator — the sample mean — can be \emph{inadmissible} when estimating a multivariate normal mean.

Let \( X \sim \mathcal{N}_p(\theta, I_p) \), where \( \theta \in \mathbb{R}^p \) is an unknown mean vector and \( I_p \) is the identity covariance matrix. The usual estimator of \( \theta \) is the sample mean:

\[
\hat{\theta}_{\text{MLE}} = X
\]

However, Charles Stein \cite{stein1956inadmissibility} showed that if the dimension \( p \geq 3 \), there exist other estimators that have uniformly lower risk (mean squared error) than \( X \). One such estimator is the James-Stein estimator:

\[
\hat{\theta}_{\text{JS}} = \left(1 - \frac{(p - 2)}{\|X\|^2} \right) X
\]

This estimator \emph{shrinks} the sample mean toward the origin, and surprisingly, this shrinkage improves the overall estimation accuracy, in terms of expected squared error:

\[
\mathbb{E}\left[ \|\hat{\theta}_{\text{JS}} - \theta\|^2 \right] < \mathbb{E}\left[ \|X - \theta\|^2 \right]
\quad \text{for all } \theta \in \mathbb{R}^p
\]

This paradoxical result highlights a key insight: when estimating multiple parameters simultaneously, borrowing strength across dimensions (via shrinkage) can yield better estimators — even in a frequentist setting.

\subsubsection{Influence on Deep Learning}

While the James--Stein estimator is not used directly in deep learning, its underlying ideas have significant conceptual and practical impact.

\paragraph{Regularization as Shrinkage}

Regularization techniques in deep learning, especially \( L^2 \) regularization (also known as weight decay), can be interpreted as forms of shrinkage:

\[
\theta \mapsto \arg\min_\theta \left\{ \frac{1}{n} \sum_{i=1}^n \ell(f_\theta(x_i), y_i) + \lambda \|\theta\|^2 \right\}
\]

This shrinks the parameter vector toward zero, akin to the James--Stein estimator. It helps reduce variance at the cost of introducing some bias, improving generalization.

\paragraph{Empirical Bayes and Bayesian Deep Learning}

The Stein Paradox inspired the development of \textbf{Empirical Bayes}, which informs various modern techniques:

\begin{itemize}
    \item \textbf{Bayesian neural networks}, where parameter estimates are drawn from posterior distributions.
    \item \textbf{Variational inference} in deep generative models (e.g., VAEs), where posterior means resemble shrinkage estimators.
    \item \textbf{Simulation-based inference}, using neural networks to estimate posteriors from data.
\end{itemize}

\paragraph{Implicit Regularization and Overparameterization}

Deep networks often operate in high-dimensional settings where parameters outnumber samples. This is exactly where Stein-type shrinkage becomes effective. Recent studies show that optimization methods like SGD induce \emph{implicit regularization}, functioning analogously to Stein-type variance control.

\paragraph{Meta-Learning and Task Adaptation}

In few-shot and meta-learning frameworks, algorithms often learn task-specific parameters that are \emph{shrunk toward a shared global mean}, mirroring James--Stein behavior:

\[
\hat{\theta}_\text{task} \leftarrow \hat{\theta}_\text{meta} + \text{adaptation step}
\]

This enables improved generalization across tasks by reducing task-specific overfitting.

\paragraph{Summary Table}

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Deep Learning Concept} & \textbf{Stein Connection} \\
\hline
L2 regularization & Shrinkage toward zero \\
Bayesian methods & Posterior means as shrinkage estimators \\
Meta-learning & Shrinkage toward global/task average \\
Early stopping, SGD bias & Implicit regularization reduces variance \\
\hline
\end{tabular}
\end{center}

The avid reader can referece to: \cite{efron2016computer, donini2021bayesian, neyshabur2017implicit}

% --------------------------------------------------------------------
\nocite{efron1977steins, lehmann1998theory, wasserman2004all}


\section{Information Quantification Through Transformations}
I'm confounding both notions as present research on AI as corrupted the original notion of latent space and now is akin to an informative embedding.

As the demand for sophisticated data analytics continues to grow, time series embedding has emerged as a significant area of study, enhancing the capabilities of machine learning models to handle complex, high-dimensional datasets while improving interpretability and generalization across domains.
 Notably, the development of embedding techniques has been influenced by advances in machine learning, particularly deep learning, leading to the emergence of methods like Variational Autoencoders (VAEs) and recurrent neural networks (RNNs), which capture temporal dependencies effectively. Additionally, the incorporation of exogenous features—external variables that may influence the target variable—has further enriched the latent space representations, allowing for more robust predictions and deeper insights into the underlying dynamics of the time series being analyzed \cite{irani2025timeseriesembeddingmethods}.
 These embeddings facilitate various applications, including anomaly detection, forecasting, and predictive analytics, across diverse sectors. However, the field is not without its challenges and controversies. Key issues include the complexities in model design, the delicate balance of dimensionality in latent space, and the interpretability of models. Moreover, ethical considerations surrounding bias and fairness in AI applications necessitate ongoing research and dialogue within the community \cite{pakazad2025timeseries, deng2024learninglatentspacesdomain}. 
 As the discipline evolves, future directions focus on improving domain generalization, optimizing regularization techniques, and integrating advanced methodologies to enhance predictive capabilities while addressing the limitations inherent in existing models.

\subsection{Methods}

\subsubsection{Neural Network-Based Methods}

\paragraph{Autoencoders} These neural networks compress time series into latent vectors by training an encoder-decoder pair to reconstruct the input data \cite{milvus2025timeseries_embeddings}. 
This approach captures the essential features of the time series while minimizing reconstruction error.

\paragraph{Recurrent Neural Networks (RNNs)} RNNs, particularly those employing Long Short-Term Memory (LSTM) units, generate embeddings by processing sequential data step-by-step. The hidden states of these networks serve as vector representations that encapsulate temporal dependencies \cite{milvus2025timeseries_embeddings, timescale2024beginners_vector_embeddings}.

\paragraph{Transformers}Leveraging self-attention mechanisms, Transformers can also produce embeddings by aggregating contextual information across time steps, making them effective in capturing long-range dependencies in time series data \cite{zilliz2025timeseries_embeddings, timescale2024beginners_vector_embeddings}.


\subsubsection{Classical Techniques}

Beyond neural networks, traditional methods such as the Fourier Transform \cite{yi2025surveydeeplearningbased, zhang2025timedomainrecentadvances}
and Wavelet Transform 
\cite{10.1007/978-3-030-61713-4_4}
have been adapted for embedding time series data. The Fourier Transform decomposes a time series into its constituent frequencies, while Wavelet Transforms provide a time-frequency representation that captures localized variations in the signal \cite{winastwan2024time_series_embedding}. These techniques are often combined with machine learning models to enhance predictive performance.

\subsection{Latent Space Representation}


Latent space representation in time series data involves creating a compressed and structured form of the original data that retains its essential features while discarding irrelevant information. This compressed representation is critical for facilitating various machine learning tasks, including forecasting, anomaly detection, and data generation.

\subsubsection{Information Preservation}
Both embeddings and latent space transformations are, in most cases, irreversible transformations. One thing to look out for then is the preservation of information; if transforming the data removes information then the representation might not capture the features needed for further analysis.
There are several ways of preventing, or at least minimizing this.


\paragraph{Shannon Entropy \cite{shannon1948mathematical}}
\begin{hangingdescription}
    \item[Definition:] Measures the average amount of information produced by a stochastic source of data.
    \item[Use:] Compare the entropy of the original waveform and the transformed spectrogram.
    \item[Limitation:] Entropy is sensitive to quantization and may not reflect perceptual quality.
    \item[Conditional Entropy:] $H(X|Y)$. Measures remaining uncertainty in $X$ given knowledge of $Y$. Can assess how much transformation explains the original series
\end{hangingdescription}
 
\paragraph{Mutual Information \cite{cover2006elements}}
\begin{hangingdescription}
    \item[Definition:] Quantifies how much information two variables share.
    \item[Use:] MI between original audio and its spectrogram version indicates retained information.
    \item[Computation:] Requires estimation of joint probability distributions, often with kernel density estimation or binning.
\end{hangingdescription}

\paragraph{Dynamic Time Warping \cite{berndt1994using}}
\begin{hangingdescription}
    \item[Description:] Measures alignment-based similarity between two sequences.
    \item[Use:] Robust to temporal shifts and warping. Useful if the transformation changes time resolution.
\end{hangingdescription}

\paragraph{Normalized Compression Distance \cite{li2004information}}
\begin{hangingdescription}
    \item[Use:] Based on the size of compressed versions. Approximate metric for shared information. 
\end{hangingdescription}

\paragraph{Fourier: Inverse Transform + Signal-to-Noise Ratio (SNR)}
\begin{hangingdescription}
    \item[Definition:] Reconstruction Error.
    \item[Approach:] Apply the inverse transformation (e.g., inverse STFT or inverse mel spectrogram) to reconstruct the waveform, then compare:
    $$
        SNR = 10 \log 10 \left( \frac{\|x\|^2}{\| x - \hat{x} \|^2} \right)
    $$
    \item[Use:] Directly quantifies fidelity of reconstruction.
    \item[Limitation:] Only useful if the Fourier Transform is relevant in your study case.
\end{hangingdescription}

\paragraph{Kullback-Leibler Divergence \cite{gray2011entropy}}
\begin{hangingdescription}
    \item[Use:] The power spectral densities (PSDs) of original and transformed signals to compute KL divergence.
    \item[Computation:] High divergence indicates distortion or information loss.
\end{hangingdescription}
   

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{|p{4cm}|p{3.2cm}|p{3.2cm}|p{2.5cm}|p{3.5cm}|}
\hline
\textbf{Metric} & \textbf{Type} & \textbf{Applicable For} & \textbf{Invertibility Required} & \textbf{Notes} \\
\hline
Shannon Entropy & Information-Theoretic & Any time series & No & Measures global uncertainty \\
\hline
Mutual Information & Information-Theoretic & Any time series & No & Measures shared content between series \\
\hline
Conditional Entropy & Information-Theoretic & Time series pairs & No & Measures remaining uncertainty given transformation \\
\hline
KL Divergence & Information-Theoretic & Distributions & No & Measures divergence between original and transformed distributions \\
\hline
Dynamic Time Warping (DTW) & Distance-Based & Temporally shifted series & No & Robust to time misalignments \\
\hline
Euclidean Distance & Distance-Based & Aligned series & No & Sensitive to noise and shifts \\
\hline
Fréchet Distance & Distance-Based & Time series paths & No & Measures curve similarity \\
\hline
Power Spectral Density (PSD) & Spectral Analysis & Periodic/trending series & No & Frequency component comparison \\
\hline
Autocorrelation / Partial ACF & Statistical Dependency & Seasonal or memory-based series & No & Captures temporal dependence structure \\
\hline
Cross-Correlation & Statistical Similarity & Two time series & No & Measures lagged similarity \\
\hline
Forecast Accuracy Comparison & Model-Based & Predictive tasks & No & Compare RMSE/MAE after transformation \\
\hline
Transfer Entropy & Information-Theoretic (Directional) & Nonlinear/dynamic systems & No & Captures asymmetric info flow \\
\hline
Reconstruction Error (MSE/MAE) & Model-Based & Encoders/Decoders & Yes & Measures signal fidelity \\
\hline
R\textsuperscript{2} Score & Model-Based & Regression fit & Yes & Proportion of variance retained \\
\hline
Normalized Compression Distance (NCD) & Compression-Based & Any time series & No & Info-based distance using compressors \\
\hline
\end{tabular}
}
\caption{Summary of Metrics for Quantifying Information Loss and Similarity in Time Series Transformations}
\label{tab:timeseries_metrics}
\end{table}


\subsubsection{Techniques and Models}
 
\paragraph{Dimensionality Reduction}
A fundamental aspect of encoding latent space is dimensionality reduction, where high-dimensional time series data is mapped into a lower-dimensional space. This process enhances computational efficiency and helps in capturing underlying patterns that are vital for model training. For example, in a time series context, each time-ordered data sequence can be transformed into a compact numerical representation, allowing models to learn from the data more effectively
\cite{milvus2025timeseries_embeddings, zilliz2025timeseries_embeddings}

\paragraph{Variational Autoencoders (VAEs)}
Variational Autoencoders (VAEs) are a powerful tool used in encoding latent space for time series data. They learn a compressed representation that is structured to follow a Gaussian distribution, thereby maintaining the organization of the latent space \cite{dhapre2024vae_timeseries_reduction}. 
This approach enables the model to reconstruct original time series data from the latent space, with reconstruction loss serving as a key performance metric, typically measured through metrics like mean squared error (MSE) \cite{dhapre2024vae_timeseries_reduction, lettieri2025time_series_rag}.
VAEs are versatile, enabling applications such as semi-supervised learning and style transfer within the time series domain \cite{shrivastava2025latentspacecharacterizationautoencoder}.

\paragraph{Temporal Dynamics}
Modeling latent dynamics in time series is crucial as it captures common patterns across different datasets, enhancing generalization to unseen data with similar temporal characteristics \cite{pakazad2025timeseries}. 
However, this task poses challenges due to the dynamic nature and inherent uncertainty of time series data. For instance, explicit relationships across timestamps can be difficult to capture, and conventional windowing techniques may lead to information loss \cite{pakazad2025timeseries}.
To mitigate these challenges, techniques such as Dynamic Time Warping can be employed to obtain distances that facilitate effective embedding in latent space \cite{10.1007/11510888_35}.

\paragraph{Visualization and Experimentation}
Visualizing the latent space is key to uncovering hidden patterns and gaining insights from time series data. Techniques such as t-SNE or PCA can be employed to visualize these representations, allowing data scientists to explore the encoded features and relationships within the data more effectively \cite{shrivastava2025latentspacecharacterizationautoencoder}. 
Experimentation with the structure and dimensionality of the latent space can lead to novel discoveries, making it an invaluable aspect of data analysis in the context of time series \cite{shrivastava2025latentspacecharacterizationautoencoder}.

\subsection{Generative AI is not only for generation}


Generative models, such as Variational Autoencoders (VAEs) \cite{Cai_2023, wang2024revisitingvaeunsupervisedtime, kingma2022autoencoding}, Generative Adversarial Networks (GANs) \cite{brophy2021generativeadversarialnetworkstime}, and autoregressive models (e.g., GPT, PixelCNN) \cite{wen2023transformerstimeseriessurvey}, are primarily designed to model the joint probability distribution $p(\mathbf{x})$ or the conditional distribution $p(\mathbf{x}|\mathbf{z})$ of observed data $\mathbf{x}$ and latent variables $\mathbf{z}$. However, their utility extends beyond generation: they can be repurposed effectively for predictive modeling, anomaly detection, and as foundational components in downstream tasks.

\paragraph*{Prediction via Conditional Modeling}

Generative models that are trained to learn $p(\mathbf{y}|\mathbf{x})$, such as conditional GANs or encoder-decoder architectures, can be directly applied to predictive tasks. For instance, in time series forecasting, an autoregressive model can learn:

\begin{equation}
    p(x_{t+1} | x_{1}, x_{2}, \ldots, x_t)
\end{equation}

This formulation enables the model to predict future states given past observations. In practice, transformers and RNN-based generative models have demonstrated strong performance in domains like weather forecasting, stock market prediction, and molecular dynamics.

\paragraph*{Anomaly Detection through Likelihood Estimation}

Generative models trained to estimate the data distribution $p(\mathbf{x})$ can be used to detect anomalous inputs that deviate significantly from the training distribution. Assuming that normal data points lie in high-probability regions, the anomaly score of an observation $\mathbf{x}^*$ can be defined as:

\begin{equation}
    \text{AnomalyScore}(\mathbf{x}^*) = -\log p(\mathbf{x}^*)
\end{equation}

Low-probability samples (i.e., high negative log-likelihood) are considered anomalous. VAEs and normalizing flows are particularly suitable for this task due to their ability to compute or approximate tractable likelihoods.

\paragraph*{Backbone for Downstream Tasks}

Modern large-scale generative models serve as powerful feature extractors or backbones for a wide range of downstream applications, including classification, clustering, and reinforcement learning. The latent representations $\mathbf{z}$ learned by models like VAEs or embeddings produced by transformers (e.g., GPT or BERT) capture high-level semantic and structural information, which can be utilized as input to discriminative models:

\begin{equation}
    \mathbf{z} = f_\theta(\mathbf{x}), \quad y = g_\phi(\mathbf{z})
\end{equation}

where $f_\theta$ is the encoder or embedding function of the generative model, and $g_\phi$ is a downstream predictor.

Additionally, pretrained generative models can be fine-tuned for specific tasks (transfer learning), or used in zero-shot and few-shot learning scenarios, demonstrating remarkable generalization capabilities.

\paragraph*{Conclusion}

The flexibility of generative models allows them to transcend their original purpose of data synthesis. By leveraging their probabilistic structure and representational power, these models play a crucial role in predictive analytics, anomaly detection, and as robust backbones for a wide range of downstream tasks in both supervised and unsupervised settings.

The reader is invited to look into these references for a background  on time series in ML and AI in general \cite{hamilton1994time, brownlee2018deep}













% --- Bibliography ---
% Use BibTeX if you have a .bib file, otherwise use the Thebibliography environment.
% Recommended for arXiv: use BibTeX locally and then copy the .bbl content into your .tex file
% before submission to ensure self-containedness.

% ----------- some other citations -------------------

% --- BibTeX Example ---
\newpage
%  \bibliographystyle{unsrt} % Or 'amsplain', 'abbrv', 'alpha', etc.
% \bibliography{bibliography} % Assuming you have your_bibliography_file.bib
\printbibliography

\end{document}