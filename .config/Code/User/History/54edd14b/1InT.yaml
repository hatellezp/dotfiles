x-vllm-server-config: &vllm_common
  image: vllm/vllm-openai:latest
  environment:
    - ${HF_TOKEN}
  restart: unless-stopped
  runtime: nvidia
  volumes:
    - hf-cache:/root/.cache/huggingface
    - ./chat_templates:/app/chat_templates:ro
  networks:
    - vllm-network

x-vllm-server-deploy-healthcheck: &vllm_health
  interval: 10s
  timeout: 5s
  retries: 30
  start_period: 60s


services:
  lgtm:
    image: grafana/otel-lgtm:latest
    container_name: lgtm
    ports:
      - "3000:3000"   # Grafana UI
      - "4317:4317"   # OTLP/gRPC
      - "4318:4318"   # OTLP/HTTP
    restart: unless-stopped
    networks:
      - vllm-network

  vllm-server-chat:
    <<: *vllm_common
    container_name: vllm_server_chat
    ports:
      - "${VLLM_CHAT_PORT}:${VLLM_CHAT_PORT}"
    command: ${SMOLM3_3B_FLAGS}
      --api-key ''
      --host 0.0.0.0
      --port ${VLLM_CHAT_PORT}
      --served_model_name "Multitool Chat"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ["0"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_CHAT_PORT}/health"]
      <<: *vllm_health
    

  vllm-server-autocomplete:
    <<: *vllm_common
    container_name: vllm_server_autocomplete
    ports:
      - "${VLLM_AUTOCOMPLETE_PORT}:${VLLM_AUTOCOMPLETE_PORT}"
    command: ${QWEN2_5CODER_1_5B_FLAGS}
      --api-key ''
      --host 0.0.0.0
      --port ${VLLM_AUTOCOMPLETE_PORT}
      --served_model_name "Multitool Autocomplete"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ["0"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_AUTOCOMPLETE_PORT}/health"]
      <<: *vllm_health

  vllm-server-code:
    <<: *vllm_common
    container_name: vllm_server_code
    ports:
      - "${VLLM_CODE_PORT}:${VLLM_CODE_PORT}"
    command: ${QWEN3CODER_30B_FLAGS}
      --api-key ''
      --host 0.0.0.0
      --port ${VLLM_CODE_PORT}
      --served_model_name "Multitool Code"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ["1"]
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:${VLLM_CODE_PORT}/health"]
      <<: *vllm_health

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: chatbot_webui
    restart: unless-stopped
    ports:
      - "${WEBUI_PORT}:8080"
    environment:
      - OPENAI_API_BASE_URL=http://vllm-server-chat:${VLLM_CHAT_PORT}/v1
      - OPENAI_API_KEY=''
      - ENABLE_OTEL=true
      - ENABLE_OTEL_METRICS=true
      - OTEL_EXPORTER_OTLP_INSECURE=true
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://lgtm:4317
      - OTEL_SERVICE_NAME=open-webui
      - OTEL_BASIC_AUTH_USERNAME=admin
      - OTEL_BASIC_AUTH_PASSWORD=admin
    depends_on:
      vllm-server-chat:
        condition: service_healthy
      vllm-server-autocomplete:
        condition: service_healthy
      vllm-server-code:
        condition: service_healthy
      lgtm:
        condition: service_started
    networks:
      - vllm-network
    volumes:
      - open-webui-data:/app/backend/data
      - hf-cache:/root/.cache/huggingface

  docs-server:
    image: nginx:alpine
    container_name: docs_server
    restart: unless-stopped
    ports:
      - "${DOCUMENTATION_PORT}:80"
    volumes:
      - ../site/:/usr/share/nginx/html:ro
    networks:
      - vllm-network

networks:
  vllm-network:
    driver: bridge
    external: true

volumes:
  open-webui-data:
    external: true
  hf-cache:
    external: true