services:
  vllm-server-chat:
    image: vllm/vllm-openai:latest
    container_name: vllm_server_chat
    restart: unless-stopped
    ports:
      - "8096:8096"
    command: >
      --model HuggingFaceTB/SmolLM3-3B
      --gpu-memory-utilization 0.7
      --host 0.0.0.0
      --port 8096
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
              device_ids: ["0"]  # Use GPU index 1
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8096/health"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 60s